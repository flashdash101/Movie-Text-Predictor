{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             original_title\n",
       "0                                    Avatar\n",
       "1  Pirates of the Caribbean: At World's End\n",
       "2                                   Spectre"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movies5000.csv', usecols=['original_title'])\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_year(title):\n",
    "    return re.sub(r'\\(\\d{4}\\)', '', title)\n",
    "\n",
    "# Apply the function to the 'titles' column\n",
    "df['title'] = df['title'].apply(remove_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Carter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>El Mariachi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>Newlyweds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>Signed, Sealed, Delivered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>Shanghai Calling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>My Date with Drew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4803 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                original_title\n",
       "0                                       Avatar\n",
       "1     Pirates of the Caribbean: At World's End\n",
       "2                                      Spectre\n",
       "3                        The Dark Knight Rises\n",
       "4                                  John Carter\n",
       "...                                        ...\n",
       "4798                               El Mariachi\n",
       "4799                                 Newlyweds\n",
       "4800                 Signed, Sealed, Delivered\n",
       "4801                          Shanghai Calling\n",
       "4802                         My Date with Drew\n",
       "\n",
       "[4803 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name = df['original_title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tokenizer to map every word with a number\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(movie_name)\n",
    "seq = tokenizer.texts_to_sequences(movie_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1564],\n",
       " [210, 2, 1, 431, 47, 432, 72],\n",
       " [1565],\n",
       " [1, 52, 211, 1566],\n",
       " [212, 601],\n",
       " [213, 8, 21],\n",
       " [1567],\n",
       " [902, 146, 2, 1568],\n",
       " [110, 214, 4, 1, 433, 53, 147],\n",
       " [173, 340, 261, 85, 2, 903]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(movie_name_half)\n",
    "seq1 = tokenizer.texts_to_sequences(movie_name_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'a': 3,\n",
       " 'and': 4,\n",
       " 'in': 5,\n",
       " '2': 6,\n",
       " 'to': 7,\n",
       " 'man': 8,\n",
       " 'i': 9,\n",
       " 'love': 10,\n",
       " 'with': 11,\n",
       " 'on': 12,\n",
       " 'me': 13,\n",
       " 'my': 14,\n",
       " 'movie': 15,\n",
       " 'you': 16,\n",
       " 'dead': 17,\n",
       " 'last': 18,\n",
       " 'for': 19,\n",
       " 'from': 20,\n",
       " '3': 21,\n",
       " 'good': 22,\n",
       " 'big': 23,\n",
       " 'day': 24,\n",
       " 'house': 25,\n",
       " 'ii': 26,\n",
       " 'american': 27,\n",
       " 'men': 28,\n",
       " 'it': 29,\n",
       " 'story': 30,\n",
       " 'life': 31,\n",
       " 'black': 32,\n",
       " 'girl': 33,\n",
       " 'all': 34,\n",
       " 'out': 35,\n",
       " 'time': 36,\n",
       " 'night': 37,\n",
       " 'is': 38,\n",
       " 'world': 39,\n",
       " 'days': 40,\n",
       " 'star': 41,\n",
       " 'part': 42,\n",
       " 'new': 43,\n",
       " 'de': 44,\n",
       " 'little': 45,\n",
       " 'la': 46,\n",
       " 'at': 47,\n",
       " 'up': 48,\n",
       " 'an': 49,\n",
       " 'die': 50,\n",
       " 'city': 51,\n",
       " 'dark': 52,\n",
       " 'blood': 53,\n",
       " 'game': 54,\n",
       " 'one': 55,\n",
       " 'red': 56,\n",
       " 'blue': 57,\n",
       " 'your': 58,\n",
       " 'iii': 59,\n",
       " 'kill': 60,\n",
       " 'war': 61,\n",
       " 'white': 62,\n",
       " 'street': 63,\n",
       " 'lost': 64,\n",
       " 'christmas': 65,\n",
       " 'legend': 66,\n",
       " 'wild': 67,\n",
       " 'how': 68,\n",
       " 'road': 69,\n",
       " 'boys': 70,\n",
       " 'high': 71,\n",
       " 'end': 72,\n",
       " 'down': 73,\n",
       " 'home': 74,\n",
       " 'hard': 75,\n",
       " 'boy': 76,\n",
       " 'year': 77,\n",
       " 'like': 78,\n",
       " 'x': 79,\n",
       " 'bad': 80,\n",
       " 'do': 81,\n",
       " 'get': 82,\n",
       " 'back': 83,\n",
       " 'about': 84,\n",
       " 'dawn': 85,\n",
       " 'trek': 86,\n",
       " 'first': 87,\n",
       " 'mr': 88,\n",
       " 'final': 89,\n",
       " 'two': 90,\n",
       " 'secret': 91,\n",
       " 'that': 92,\n",
       " 'we': 93,\n",
       " 'no': 94,\n",
       " 'dragon': 95,\n",
       " 'planet': 96,\n",
       " 'mad': 97,\n",
       " 'earth': 98,\n",
       " 'never': 99,\n",
       " 'land': 100,\n",
       " 'return': 101,\n",
       " 'spy': 102,\n",
       " 'les': 103,\n",
       " 'evil': 104,\n",
       " 'death': 105,\n",
       " 'by': 106,\n",
       " 'le': 107,\n",
       " 'who': 108,\n",
       " 'friday': 109,\n",
       " 'harry': 110,\n",
       " 'fire': 111,\n",
       " 'escape': 112,\n",
       " '4': 113,\n",
       " 'four': 114,\n",
       " 'what': 115,\n",
       " 'this': 116,\n",
       " 'dog': 117,\n",
       " 'run': 118,\n",
       " 'best': 119,\n",
       " 'thing': 120,\n",
       " 'king': 121,\n",
       " 'america': 122,\n",
       " 'green': 123,\n",
       " 'vs': 124,\n",
       " 'rise': 125,\n",
       " 'snow': 126,\n",
       " 'after': 127,\n",
       " 'ghost': 128,\n",
       " 'perfect': 129,\n",
       " 'or': 130,\n",
       " 'island': 131,\n",
       " 'sea': 132,\n",
       " 'york': 133,\n",
       " 'state': 134,\n",
       " 'over': 135,\n",
       " 'go': 136,\n",
       " 'young': 137,\n",
       " 'tale': 138,\n",
       " 'be': 139,\n",
       " 'eyes': 140,\n",
       " 'water': 141,\n",
       " 'her': 142,\n",
       " 'cop': 143,\n",
       " 'baby': 144,\n",
       " 'gun': 145,\n",
       " 'age': 146,\n",
       " 'prince': 147,\n",
       " 'jones': 148,\n",
       " 'beyond': 149,\n",
       " 'joe': 150,\n",
       " 'inside': 151,\n",
       " 'not': 152,\n",
       " 'adventures': 153,\n",
       " 'd': 154,\n",
       " 'true': 155,\n",
       " 'just': 156,\n",
       " 'space': 157,\n",
       " 'brothers': 158,\n",
       " 'rock': 159,\n",
       " 'money': 160,\n",
       " 'got': 161,\n",
       " 'club': 162,\n",
       " 'family': 163,\n",
       " 'wedding': 164,\n",
       " 'our': 165,\n",
       " 'guy': 166,\n",
       " 'woman': 167,\n",
       " 'du': 168,\n",
       " 'when': 169,\n",
       " 'next': 170,\n",
       " 'girls': 171,\n",
       " 'saw': 172,\n",
       " 'batman': 173,\n",
       " 'monsters': 174,\n",
       " 'great': 175,\n",
       " 'jack': 176,\n",
       " 'moon': 177,\n",
       " 'witch': 178,\n",
       " 'book': 179,\n",
       " 'journey': 180,\n",
       " 'max': 181,\n",
       " '1': 182,\n",
       " '13th': 183,\n",
       " 'heaven': 184,\n",
       " 'other': 185,\n",
       " 'things': 186,\n",
       " 'el': 187,\n",
       " 'ice': 188,\n",
       " \"don't\": 189,\n",
       " \"devil's\": 190,\n",
       " 'hotel': 191,\n",
       " 'dogs': 192,\n",
       " 'e': 193,\n",
       " 'kid': 194,\n",
       " 'glory': 195,\n",
       " 'way': 196,\n",
       " 'hills': 197,\n",
       " 'dream': 198,\n",
       " 'drive': 199,\n",
       " 'room': 200,\n",
       " 'child': 201,\n",
       " 'texas': 202,\n",
       " 'hot': 203,\n",
       " 'midnight': 204,\n",
       " '3d': 205,\n",
       " 'diary': 206,\n",
       " 'school': 207,\n",
       " 'summer': 208,\n",
       " 'light': 209,\n",
       " 'pirates': 210,\n",
       " 'knight': 211,\n",
       " 'john': 212,\n",
       " 'spider': 213,\n",
       " 'potter': 214,\n",
       " 'iron': 215,\n",
       " 'revenge': 216,\n",
       " 'into': 217,\n",
       " 'darkness': 218,\n",
       " 'apes': 219,\n",
       " 'train': 220,\n",
       " 'games': 221,\n",
       " 'angels': 222,\n",
       " 'mission': 223,\n",
       " 'code': 224,\n",
       " 'know': 225,\n",
       " 'wars': 226,\n",
       " 'town': 227,\n",
       " 'are': 228,\n",
       " 'son': 229,\n",
       " 'heart': 230,\n",
       " 'sex': 231,\n",
       " 'park': 232,\n",
       " 'three': 233,\n",
       " 'meet': 234,\n",
       " 'deep': 235,\n",
       " 'eye': 236,\n",
       " 'seven': 237,\n",
       " 'bride': 238,\n",
       " 's': 239,\n",
       " '13': 240,\n",
       " 'kiss': 241,\n",
       " 'show': 242,\n",
       " 'michael': 243,\n",
       " 'under': 244,\n",
       " 'miss': 245,\n",
       " 'did': 246,\n",
       " 'super': 247,\n",
       " 'silent': 248,\n",
       " 'destination': 249,\n",
       " 'soul': 250,\n",
       " 'party': 251,\n",
       " 'nightmare': 252,\n",
       " 'walk': 253,\n",
       " 'too': 254,\n",
       " 'romeo': 255,\n",
       " 'call': 256,\n",
       " 'valley': 257,\n",
       " 'before': 258,\n",
       " 'married': 259,\n",
       " 'halloween': 260,\n",
       " 'superman': 261,\n",
       " 'battle': 262,\n",
       " 'five': 263,\n",
       " 'hood': 264,\n",
       " 'captain': 265,\n",
       " 'kingdom': 266,\n",
       " 'rush': 267,\n",
       " 'edge': 268,\n",
       " 'tomorrow': 269,\n",
       " 'mars': 270,\n",
       " 'wanted': 271,\n",
       " 'side': 272,\n",
       " 'happy': 273,\n",
       " 'stone': 274,\n",
       " 'curse': 275,\n",
       " 'jason': 276,\n",
       " 'teenage': 277,\n",
       " 'ninja': 278,\n",
       " 'real': 279,\n",
       " 'live': 280,\n",
       " 'princess': 281,\n",
       " 'country': 282,\n",
       " 'lies': 283,\n",
       " 'wolf': 284,\n",
       " 'now': 285,\n",
       " \"it's\": 286,\n",
       " 'open': 287,\n",
       " 'nights': 288,\n",
       " 'mighty': 289,\n",
       " 'walking': 290,\n",
       " 'hunter': 291,\n",
       " 'u': 292,\n",
       " 'l': 293,\n",
       " 'monster': 294,\n",
       " 'people': 295,\n",
       " 'lady': 296,\n",
       " 'vampire': 297,\n",
       " 'women': 298,\n",
       " 'step': 299,\n",
       " 'date': 300,\n",
       " 'broken': 301,\n",
       " 'long': 302,\n",
       " 'adventure': 303,\n",
       " 'lucky': 304,\n",
       " 'runner': 305,\n",
       " 'shadow': 306,\n",
       " 'london': 307,\n",
       " 'magic': 308,\n",
       " 'name': 309,\n",
       " 'vegas': 310,\n",
       " 'line': 311,\n",
       " 'paris': 312,\n",
       " '8': 313,\n",
       " '10': 314,\n",
       " 'crazy': 315,\n",
       " 'hill': 316,\n",
       " 'beverly': 317,\n",
       " 'work': 318,\n",
       " 'fat': 319,\n",
       " 'river': 320,\n",
       " 'dangerous': 321,\n",
       " 'kids': 322,\n",
       " 'again': 323,\n",
       " 'living': 324,\n",
       " 'lake': 325,\n",
       " 'sweet': 326,\n",
       " 'devil': 327,\n",
       " 'friends': 328,\n",
       " 'bang': 329,\n",
       " 'take': 330,\n",
       " 'greatest': 331,\n",
       " 'lives': 332,\n",
       " 'goes': 333,\n",
       " 'have': 334,\n",
       " 'top': 335,\n",
       " 'il': 336,\n",
       " 'brown': 337,\n",
       " 'paranormal': 338,\n",
       " 'activity': 339,\n",
       " 'v': 340,\n",
       " 'lone': 341,\n",
       " 'chronicles': 342,\n",
       " 'golden': 343,\n",
       " 'terminator': 344,\n",
       " 'hour': 345,\n",
       " 'west': 346,\n",
       " 'tomb': 347,\n",
       " 'forever': 348,\n",
       " 'fast': 349,\n",
       " 'hunger': 350,\n",
       " 'charlie': 351,\n",
       " 'force': 352,\n",
       " 'impossible': 353,\n",
       " 'away': 354,\n",
       " 'most': 355,\n",
       " 'another': 356,\n",
       " 'dick': 357,\n",
       " 'twilight': 358,\n",
       " 'bourne': 359,\n",
       " 'mutant': 360,\n",
       " 'lord': 361,\n",
       " 'gone': 362,\n",
       " 'where': 363,\n",
       " 'point': 364,\n",
       " 'zone': 365,\n",
       " 'thief': 366,\n",
       " 'saint': 367,\n",
       " 'air': 368,\n",
       " 'holiday': 369,\n",
       " 'cold': 370,\n",
       " 'pink': 371,\n",
       " 'still': 372,\n",
       " 'machine': 373,\n",
       " 'et': 374,\n",
       " 'don': 375,\n",
       " 'league': 376,\n",
       " 'hollywood': 377,\n",
       " 'funny': 378,\n",
       " 'dr': 379,\n",
       " 'alien': 380,\n",
       " 'soldiers': 381,\n",
       " 'los': 382,\n",
       " 'rain': 383,\n",
       " 'plan': 384,\n",
       " 'hours': 385,\n",
       " 'blade': 386,\n",
       " 'resident': 387,\n",
       " 'race': 388,\n",
       " 'play': 389,\n",
       " 'rising': 390,\n",
       " 'shanghai': 391,\n",
       " 'catch': 392,\n",
       " 'can': 393,\n",
       " 'stop': 394,\n",
       " 'number': 395,\n",
       " 'list': 396,\n",
       " 'along': 397,\n",
       " 'here': 398,\n",
       " 'double': 399,\n",
       " 'scream': 400,\n",
       " 'ride': 401,\n",
       " 'bridge': 402,\n",
       " '5': 403,\n",
       " 'art': 404,\n",
       " 'sunshine': 405,\n",
       " 'truth': 406,\n",
       " 'burn': 407,\n",
       " 'ghosts': 408,\n",
       " 'she': 409,\n",
       " 'there': 410,\n",
       " 'elm': 411,\n",
       " '40': 412,\n",
       " 'old': 413,\n",
       " 'queen': 414,\n",
       " 'will': 415,\n",
       " '9': 416,\n",
       " 'without': 417,\n",
       " '30': 418,\n",
       " 'running': 419,\n",
       " 'der': 420,\n",
       " 'cry': 421,\n",
       " 'welcome': 422,\n",
       " 'brother': 423,\n",
       " 'nothing': 424,\n",
       " \"she's\": 425,\n",
       " 'lights': 426,\n",
       " 'kind': 427,\n",
       " 'butterfly': 428,\n",
       " 'being': 429,\n",
       " 'they': 430,\n",
       " 'caribbean': 431,\n",
       " \"world's\": 432,\n",
       " 'half': 433,\n",
       " 'returns': 434,\n",
       " \"man's\": 435,\n",
       " 'steel': 436,\n",
       " 'stranger': 437,\n",
       " 'robin': 438,\n",
       " 'jurassic': 439,\n",
       " 'transformers': 440,\n",
       " 'oz': 441,\n",
       " 'furious': 442,\n",
       " 'z': 443,\n",
       " 'future': 444,\n",
       " 'past': 445,\n",
       " 'lion': 446,\n",
       " 'aliens': 447,\n",
       " 'g': 448,\n",
       " 'jungle': 449,\n",
       " 'soldier': 450,\n",
       " 'shrek': 451,\n",
       " 'hero': 452,\n",
       " 'glass': 453,\n",
       " 'am': 454,\n",
       " 'chocolate': 455,\n",
       " 'madagascar': 456,\n",
       " 'frozen': 457,\n",
       " 'wrath': 458,\n",
       " 'shadows': 459,\n",
       " 'kung': 460,\n",
       " 'jane': 461,\n",
       " 'gods': 462,\n",
       " 'kings': 463,\n",
       " 'master': 464,\n",
       " 'rider': 465,\n",
       " 'speed': 466,\n",
       " 'inc': 467,\n",
       " 'turtles': 468,\n",
       " 'mrs': 469,\n",
       " 'empire': 470,\n",
       " 'free': 471,\n",
       " 'rings': 472,\n",
       " 'groove': 473,\n",
       " 'national': 474,\n",
       " 'mask': 475,\n",
       " 'lovely': 476,\n",
       " 'vengeance': 477,\n",
       " 'alvin': 478,\n",
       " 'chipmunks': 479,\n",
       " 'haunted': 480,\n",
       " 'see': 481,\n",
       " 'own': 482,\n",
       " 'safe': 483,\n",
       " 'thirteen': 484,\n",
       " 'action': 485,\n",
       " 'stories': 486,\n",
       " 'haunting': 487,\n",
       " 'dreams': 488,\n",
       " 'nine': 489,\n",
       " 'sound': 490,\n",
       " 'beautiful': 491,\n",
       " 'mind': 492,\n",
       " 'beginning': 493,\n",
       " 'angry': 494,\n",
       " 'ballad': 495,\n",
       " 'bobby': 496,\n",
       " 't': 497,\n",
       " 'sky': 498,\n",
       " 'predator': 499,\n",
       " 'fish': 500,\n",
       " 'horse': 501,\n",
       " 'dracula': 502,\n",
       " 'years': 503,\n",
       " 'company': 504,\n",
       " 'underworld': 505,\n",
       " 'hall': 506,\n",
       " 'body': 507,\n",
       " 'saving': 508,\n",
       " 'want': 509,\n",
       " 'santa': 510,\n",
       " 'trade': 511,\n",
       " 'sin': 512,\n",
       " 'hearts': 513,\n",
       " 'chicken': 514,\n",
       " 'daddy': 515,\n",
       " 'job': 516,\n",
       " 'close': 517,\n",
       " 'joy': 518,\n",
       " 'trouble': 519,\n",
       " 'george': 520,\n",
       " 'bill': 521,\n",
       " 'chain': 522,\n",
       " 'un': 523,\n",
       " 'head': 524,\n",
       " 'godfather': 525,\n",
       " 'zero': 526,\n",
       " 'as': 527,\n",
       " 'say': 528,\n",
       " 'us': 529,\n",
       " 'fair': 530,\n",
       " 'sight': 531,\n",
       " 'riding': 532,\n",
       " 'liar': 533,\n",
       " 'scary': 534,\n",
       " 'hell': 535,\n",
       " 'tin': 536,\n",
       " 'johnny': 537,\n",
       " 'st': 538,\n",
       " '21': 539,\n",
       " 'came': 540,\n",
       " 'minutes': 541,\n",
       " 'dance': 542,\n",
       " 'honor': 543,\n",
       " 'pitch': 544,\n",
       " 'november': 545,\n",
       " 'mean': 546,\n",
       " 'streets': 547,\n",
       " 'something': 548,\n",
       " 'wife': 549,\n",
       " 'so': 550,\n",
       " 'virgin': 551,\n",
       " 'fear': 552,\n",
       " 'radio': 553,\n",
       " 'must': 554,\n",
       " 'business': 555,\n",
       " 'freedom': 556,\n",
       " 'history': 557,\n",
       " 'letters': 558,\n",
       " \"what's\": 559,\n",
       " 'chasing': 560,\n",
       " 'once': 561,\n",
       " 'grace': 562,\n",
       " 'straight': 563,\n",
       " 'only': 564,\n",
       " 'luck': 565,\n",
       " 'sisterhood': 566,\n",
       " 'right': 567,\n",
       " 'blonde': 568,\n",
       " 'crocodile': 569,\n",
       " 'dundee': 570,\n",
       " 'ever': 571,\n",
       " 'but': 572,\n",
       " 'god': 573,\n",
       " 'omega': 574,\n",
       " 'hit': 575,\n",
       " 'alone': 576,\n",
       " 'jackass': 577,\n",
       " 'let': 578,\n",
       " 'rules': 579,\n",
       " 'dancer': 580,\n",
       " 'das': 581,\n",
       " 'enter': 582,\n",
       " 'everything': 583,\n",
       " 'nowhere': 584,\n",
       " 'born': 585,\n",
       " 'valentine': 586,\n",
       " 'brooklyn': 587,\n",
       " 'dirty': 588,\n",
       " 'pet': 589,\n",
       " 'b': 590,\n",
       " 'musical': 591,\n",
       " 'chapter': 592,\n",
       " 'low': 593,\n",
       " 'better': 594,\n",
       " 'wind': 595,\n",
       " 'saints': 596,\n",
       " 'wicked': 597,\n",
       " 'des': 598,\n",
       " '20': 599,\n",
       " 'ha': 600,\n",
       " 'carter': 601,\n",
       " 'narnia': 602,\n",
       " 'hobbit': 603,\n",
       " 'amazing': 604,\n",
       " 'titanic': 605,\n",
       " 'civil': 606,\n",
       " 'alice': 607,\n",
       " 'wonderland': 608,\n",
       " 'stand': 609,\n",
       " 'fallen': 610,\n",
       " 'cars': 611,\n",
       " 'toy': 612,\n",
       " 'indiana': 613,\n",
       " 'brave': 614,\n",
       " 'carol': 615,\n",
       " 'apocalypse': 616,\n",
       " 'lovers': 617,\n",
       " '6': 618,\n",
       " 'express': 619,\n",
       " 'independence': 620,\n",
       " 'guardians': 621,\n",
       " 'galaxy': 622,\n",
       " 'through': 623,\n",
       " 'looking': 624,\n",
       " 'voyage': 625,\n",
       " 'pearl': 626,\n",
       " 'factory': 627,\n",
       " 'africa': 628,\n",
       " 'museum': 629,\n",
       " 'matrix': 630,\n",
       " 'titans': 631,\n",
       " 'nation': 632,\n",
       " 'fu': 633,\n",
       " 'panda': 634,\n",
       " 'samurai': 635,\n",
       " 'weapon': 636,\n",
       " 'avenger': 637,\n",
       " 'enough': 638,\n",
       " 'far': 639,\n",
       " 'saga': 640,\n",
       " 'feet': 641,\n",
       " 'penguins': 642,\n",
       " 'prisoner': 643,\n",
       " 'r': 644,\n",
       " 'sherlock': 645,\n",
       " 'holmes': 646,\n",
       " 'warrior': 647,\n",
       " 'storm': 648,\n",
       " 'surfer': 649,\n",
       " 'full': 650,\n",
       " 'episode': 651,\n",
       " 'phantom': 652,\n",
       " \"winter's\": 653,\n",
       " \"ocean's\": 654,\n",
       " 'smith': 655,\n",
       " 'range': 656,\n",
       " 'control': 657,\n",
       " 'cat': 658,\n",
       " 'hat': 659,\n",
       " 'martian': 660,\n",
       " 'secrets': 661,\n",
       " 'casino': 662,\n",
       " 'expendables': 663,\n",
       " 'wall': 664,\n",
       " 'atlas': 665,\n",
       " 'break': 666,\n",
       " 'cinderella': 667,\n",
       " 'finding': 668,\n",
       " 'jackson': 669,\n",
       " 'beneath': 670,\n",
       " 'thunder': 671,\n",
       " 'cradle': 672,\n",
       " 'k': 673,\n",
       " 'heist': 674,\n",
       " 'enemy': 675,\n",
       " 'mirror': 676,\n",
       " 'cats': 677,\n",
       " 'impact': 678,\n",
       " 'longest': 679,\n",
       " 'give': 680,\n",
       " 'face': 681,\n",
       " 'off': 682,\n",
       " 'mountain': 683,\n",
       " 'conspiracy': 684,\n",
       " 'bear': 685,\n",
       " 'spirit': 686,\n",
       " '2000': 687,\n",
       " 'may': 688,\n",
       " 'come': 689,\n",
       " 'rocky': 690,\n",
       " 'extraordinary': 691,\n",
       " '50': 692,\n",
       " 'exorcist': 693,\n",
       " 'shark': 694,\n",
       " 'cool': 695,\n",
       " 'sun': 696,\n",
       " 'killers': 697,\n",
       " 'n': 698,\n",
       " 'anna': 699,\n",
       " 'flight': 700,\n",
       " 'punch': 701,\n",
       " 'gold': 702,\n",
       " 'doctor': 703,\n",
       " 'were': 704,\n",
       " 'has': 705,\n",
       " 'siege': 706,\n",
       " 'killer': 707,\n",
       " 'legends': 708,\n",
       " 'mystery': 709,\n",
       " 'lincoln': 710,\n",
       " 'private': 711,\n",
       " 'due': 712,\n",
       " 'center': 713,\n",
       " 'annie': 714,\n",
       " 'fight': 715,\n",
       " 'marshall': 716,\n",
       " 'frankenstein': 717,\n",
       " 'austin': 718,\n",
       " 'powers': 719,\n",
       " 'eight': 720,\n",
       " 'mile': 721,\n",
       " 'identity': 722,\n",
       " 'daughter': 723,\n",
       " 'weeks': 724,\n",
       " 'maze': 725,\n",
       " 'sunday': 726,\n",
       " 'resurrection': 727,\n",
       " '51': 728,\n",
       " 'mortal': 729,\n",
       " 'holy': 730,\n",
       " 'tall': 731,\n",
       " 'pursuit': 732,\n",
       " 'vol': 733,\n",
       " 'tango': 734,\n",
       " 'elizabeth': 735,\n",
       " 'muppets': 736,\n",
       " \"king's\": 737,\n",
       " 'law': 738,\n",
       " 'lose': 739,\n",
       " 'interview': 740,\n",
       " 'loaded': 741,\n",
       " 'ground': 742,\n",
       " 'beach': 743,\n",
       " 'raising': 744,\n",
       " 'steve': 745,\n",
       " 'letter': 746,\n",
       " 'fighter': 747,\n",
       " 'boat': 748,\n",
       " 'bone': 749,\n",
       " 'taken': 750,\n",
       " 'cable': 751,\n",
       " 'mary': 752,\n",
       " 'diaries': 753,\n",
       " 'royal': 754,\n",
       " 'quest': 755,\n",
       " 'english': 756,\n",
       " 'pretty': 757,\n",
       " 'miracle': 758,\n",
       " 'bounty': 759,\n",
       " 'gate': 760,\n",
       " '28': 761,\n",
       " 'sense': 762,\n",
       " 'fifty': 763,\n",
       " 'shades': 764,\n",
       " 'their': 765,\n",
       " 'network': 766,\n",
       " 'julia': 767,\n",
       " 'dumb': 768,\n",
       " 'below': 769,\n",
       " 'greek': 770,\n",
       " 'small': 771,\n",
       " 'horrible': 772,\n",
       " 'skeleton': 773,\n",
       " 'made': 774,\n",
       " 'music': 775,\n",
       " 'paul': 776,\n",
       " 'trip': 777,\n",
       " 'million': 778,\n",
       " 'morning': 779,\n",
       " 'extreme': 780,\n",
       " 'words': 781,\n",
       " 'grudge': 782,\n",
       " 'match': 783,\n",
       " 'single': 784,\n",
       " 'dolphin': 785,\n",
       " \"we're\": 786,\n",
       " 'dragons': 787,\n",
       " 'sorority': 788,\n",
       " 'banks': 789,\n",
       " 'lions': 790,\n",
       " 'femme': 791,\n",
       " 'desert': 792,\n",
       " 'father': 793,\n",
       " 'alive': 794,\n",
       " 'transporter': 795,\n",
       " 'attraction': 796,\n",
       " 'repo': 797,\n",
       " 'vacation': 798,\n",
       " 'remember': 799,\n",
       " 'hunt': 800,\n",
       " 'calls': 801,\n",
       " 'dollar': 802,\n",
       " 'rugrats': 803,\n",
       " 'fall': 804,\n",
       " 'hope': 805,\n",
       " 'sarah': 806,\n",
       " 'bottle': 807,\n",
       " 'mike': 808,\n",
       " 'naked': 809,\n",
       " 'rabbit': 810,\n",
       " 'yards': 811,\n",
       " 'pride': 812,\n",
       " 'cave': 813,\n",
       " 'united': 814,\n",
       " 'faith': 815,\n",
       " 'woodstock': 816,\n",
       " 'kick': 817,\n",
       " 'very': 818,\n",
       " 'prejudice': 819,\n",
       " 'door': 820,\n",
       " 'zombies': 821,\n",
       " 'vi': 822,\n",
       " 'blues': 823,\n",
       " 'noise': 824,\n",
       " 'happened': 825,\n",
       " 'o': 826,\n",
       " 'agent': 827,\n",
       " 'cody': 828,\n",
       " 'highlander': 829,\n",
       " 'jimmy': 830,\n",
       " 'genius': 831,\n",
       " 'bunny': 832,\n",
       " 'beauty': 833,\n",
       " 'grave': 834,\n",
       " 'august': 835,\n",
       " 'tora': 836,\n",
       " 'dear': 837,\n",
       " 'bright': 838,\n",
       " 'lords': 839,\n",
       " 'iv': 840,\n",
       " 'think': 841,\n",
       " 'fisher': 842,\n",
       " 'march': 843,\n",
       " 'jaws': 844,\n",
       " 'animal': 845,\n",
       " 'wimpy': 846,\n",
       " 'funeral': 847,\n",
       " 'molly': 848,\n",
       " 'song': 849,\n",
       " '12': 850,\n",
       " 'hole': 851,\n",
       " 'police': 852,\n",
       " 'chainsaw': 853,\n",
       " 'harold': 854,\n",
       " 'kumar': 855,\n",
       " 'psycho': 856,\n",
       " 'kills': 857,\n",
       " \"can't\": 858,\n",
       " 'horror': 859,\n",
       " 'las': 860,\n",
       " 'exorcism': 861,\n",
       " 'rose': 862,\n",
       " 'happens': 863,\n",
       " 'station': 864,\n",
       " 'anything': 865,\n",
       " \"let's\": 866,\n",
       " 'tales': 867,\n",
       " 'massacre': 868,\n",
       " 'teen': 869,\n",
       " 'cut': 870,\n",
       " 'place': 871,\n",
       " 'voice': 872,\n",
       " 'ones': 873,\n",
       " 'dude': 874,\n",
       " 'car': 875,\n",
       " 'effect': 876,\n",
       " 'project': 877,\n",
       " 'always': 878,\n",
       " 'purge': 879,\n",
       " 'insidious': 880,\n",
       " 'sugar': 881,\n",
       " 'heroes': 882,\n",
       " 'film': 883,\n",
       " 'hai': 884,\n",
       " 'ë†ˆ': 885,\n",
       " '000': 886,\n",
       " 'food': 887,\n",
       " 'falls': 888,\n",
       " 'kevin': 889,\n",
       " 'mother': 890,\n",
       " 'h': 891,\n",
       " 'east': 892,\n",
       " 'myers': 893,\n",
       " 'was': 894,\n",
       " 'shine': 895,\n",
       " 'lane': 896,\n",
       " 'billy': 897,\n",
       " 'together': 898,\n",
       " 'times': 899,\n",
       " 'giants': 900,\n",
       " 'roadside': 901,\n",
       " 'avengers': 902,\n",
       " 'justice': 903,\n",
       " 'ranger': 904,\n",
       " 'tides': 905,\n",
       " 'battleship': 906,\n",
       " 'extinction': 907,\n",
       " 'legacy': 908,\n",
       " 'salvation': 909,\n",
       " '7': 910,\n",
       " 'giant': 911,\n",
       " 'slayer': 912,\n",
       " 'sands': 913,\n",
       " 'dinosaur': 914,\n",
       " 'mummy': 915,\n",
       " 'emperor': 916,\n",
       " 'suicide': 917,\n",
       " 'squad': 918,\n",
       " 'almighty': 919,\n",
       " 'huntsman': 920,\n",
       " 'ronin': 921,\n",
       " 'winter': 922,\n",
       " 'ralph': 923,\n",
       " 'curious': 924,\n",
       " 'case': 925,\n",
       " 'benjamin': 926,\n",
       " 'class': 927,\n",
       " 'mockingjay': 928,\n",
       " 'third': 929,\n",
       " 'alexander': 930,\n",
       " 'order': 931,\n",
       " 'phoenix': 932,\n",
       " 'begins': 933,\n",
       " 'origins': 934,\n",
       " 'wolverine': 935,\n",
       " 'thor': 936,\n",
       " 'fury': 937,\n",
       " 'bee': 938,\n",
       " 'moms': 939,\n",
       " 'armageddon': 940,\n",
       " 'fun': 941,\n",
       " 'egypt': 942,\n",
       " 'lethal': 943,\n",
       " 'hulk': 944,\n",
       " 'within': 945,\n",
       " 'commander': 946,\n",
       " 'breaking': 947,\n",
       " 'incredible': 948,\n",
       " 'ant': 949,\n",
       " 'worlds': 950,\n",
       " 'p': 951,\n",
       " 'da': 952,\n",
       " 'rio': 953,\n",
       " 'silver': 954,\n",
       " 'pi': 955,\n",
       " \"charlie's\": 956,\n",
       " 'stuart': 957,\n",
       " 'riddick': 958,\n",
       " 'robocop': 959,\n",
       " 'attack': 960,\n",
       " 'menace': 961,\n",
       " 'gravity': 962,\n",
       " 'fantastic': 963,\n",
       " 'san': 964,\n",
       " 'patriot': 965,\n",
       " '300': 966,\n",
       " 'smurfs': 967,\n",
       " 'ring': 968,\n",
       " 'robot': 969,\n",
       " 'report': 970,\n",
       " 'judgment': 971,\n",
       " 'gangster': 972,\n",
       " 'taking': 973,\n",
       " 'fockers': 974,\n",
       " 'guys': 975,\n",
       " 'dame': 976,\n",
       " \"emperor's\": 977,\n",
       " 'treasure': 978,\n",
       " 'epic': 979,\n",
       " 'diamond': 980,\n",
       " 'troopers': 981,\n",
       " 'cloud': 982,\n",
       " 'hercules': 983,\n",
       " 'wives': 984,\n",
       " 'hawk': 985,\n",
       " 'fifth': 986,\n",
       " 'drift': 987,\n",
       " 'bones': 988,\n",
       " 'lara': 989,\n",
       " 'croft': 990,\n",
       " 'raider': 991,\n",
       " 'percy': 992,\n",
       " 'cloudy': 993,\n",
       " 'chance': 994,\n",
       " 'meatballs': 995,\n",
       " 'dinosaurs': 996,\n",
       " 'walter': 997,\n",
       " 'tattoo': 998,\n",
       " 'atlantis': 999,\n",
       " 'intelligence': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index\n",
    "#print index for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words dropped are:  1003\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "#X contains the input data and y contains the output data\n",
    "total_words_dropped = 0\n",
    "for i in seq:\n",
    "    if len(i) > 1:\n",
    "        for index in range(1,len(i)):\n",
    "            x.append(i[:index])\n",
    "            y.append(i[index])\n",
    "    else:\n",
    "        total_words_dropped +=1\n",
    "print('Total words dropped are: ', total_words_dropped)\n",
    "#Dropping the words of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[210],\n",
       " [210, 2],\n",
       " [210, 2, 1],\n",
       " [210, 2, 1, 431],\n",
       " [210, 2, 1, 431, 47],\n",
       " [210, 2, 1, 431, 47, 432],\n",
       " [1],\n",
       " [1, 52],\n",
       " [1, 52, 211],\n",
       " [212]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8483, 5045)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.preprocessing.sequence.pad_sequences(x)\n",
    "# x is a vector of sequences that have been padded to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y)\n",
    "#y is now a one-hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#vocab_size is the number of unique words in the vocabulary + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8483, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "#debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5045"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers, optimizers,Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size,14))\n",
    "model.add(layers.LSTM(160, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.LSTM(120))\n",
    "model.add(layers.Dense(120, activation = 'relu'))\n",
    "model.add(layers.Dense(vocab_size, activation='softmax'))\n",
    "#Create the model using Recurrent Neural Network, Long Short-Term Memory, and Dropout layers to help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, None, 14)          70630     \n",
      "                                                                 \n",
      " lstm_28 (LSTM)              (None, None, 160)         112000    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 160)         0         \n",
      "                                                                 \n",
      " lstm_29 (LSTM)              (None, 120)               134880    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 5045)              610445    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 942475 (3.60 MB)\n",
      "Trainable params: 942475 (3.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#Getting the summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.06), \n",
    "    loss = 'categorical_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.test_util.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 3s 499ms/step - loss: 1.8484 - accuracy: 0.5760\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 422ms/step - loss: 1.8266 - accuracy: 0.5759\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 449ms/step - loss: 1.8448 - accuracy: 0.5754\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 1.8089 - accuracy: 0.5757\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 1.7718 - accuracy: 0.5790\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 429ms/step - loss: 1.7726 - accuracy: 0.5860\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 419ms/step - loss: 1.7760 - accuracy: 0.5816\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 1.7620 - accuracy: 0.5807\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 412ms/step - loss: 1.8054 - accuracy: 0.5740\n",
      "Epoch 10/50\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.8397 - accuracy: 0.5679"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(x,y, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m , batch_size\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\adesi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x,y, epochs=50 , batch_size=2000)\n",
    "#Training the model with 50 epochs and batch size of 2000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextPredict2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextPredict2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('TextPredict2')\n",
    "#Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('TextPredict2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vocab_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index\u001b[39m.\u001b[39mkeys()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_array = np.array(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'of', 'a', ..., 'signed', 'sealed', 'delivered'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Initialize the learning rate schedule\n",
    "initial_learning_rate = 0.003\n",
    "lr_schedule = InverseTimeDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule, clipvalue=1.0)  # Add gradient clipping\n",
    "\n",
    "# Compile the model with the new optimizer\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Text: Pirates of\n",
      "Predictions: ['the', 'an', 'you']\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Text: Pirates of the\n",
      "Predictions: ['caribbean', 'intruder', 'phoenix']\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Text: Pirates of the caribbean\n",
      "Predictions: ['the', 'at', 'girl']\n",
      "Generated Text: Pirates of the caribbean the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_pred(text, n_words):\n",
    "    for _ in range(n_words):  # Use an underscore for an unused loop variable\n",
    "        text_tokenize = tokenizer.texts_to_sequences([text])[0]\n",
    "        text_padded = tf.keras.preprocessing.sequence.pad_sequences([text_tokenize], maxlen=14)\n",
    "        \n",
    "        # Predict the next word\n",
    "        prediction = model.predict(text_padded)\n",
    "        \n",
    "        # Get the top 3 predicted word indices\n",
    "        top_indices = np.argsort(prediction[0])[-3:][::-1]\n",
    "        \n",
    "        # Get the corresponding words from the vocabulary\n",
    "        top_words = [vocab_array[index - 1] for index in top_indices]\n",
    "        \n",
    "        # Print the current text and top predictions\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Predictions: {top_words}\")\n",
    "        \n",
    "        # Choose one of the top predictions (e.g., the first one) as the next word\n",
    "        next_word = top_words[0]\n",
    "        \n",
    "        # Append the next word to the text\n",
    "        text += \" \" + next_word\n",
    "        \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "generated_text = make_pred(\"Pirates of\", 3)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Text: Pirates of\n",
      "Predictions: ['the', 'an', 'you']\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Text: Pirates of the\n",
      "Predictions: ['caribbean', 'intruder', 'phoenix']\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Text: Pirates of the caribbean\n",
      "Predictions: ['the', 'at', 'girl']\n",
      "Generated Text: Pirates of the caribbean the\n"
     ]
    }
   ],
   "source": [
    "generated_text = make_pred(\"Pirates of\", 3)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Did you mean 'sp1der man of'?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def suggest_next_word(input_text):\n",
    "    text_tokenize = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    text_padded = tf.keras.preprocessing.sequence.pad_sequences([text_tokenize], maxlen=14)\n",
    "    \n",
    "    # Predict the next word\n",
    "    prediction = model.predict(text_padded)\n",
    "    \n",
    "    # Get the top predicted word index\n",
    "    top_index = np.argmax(prediction)\n",
    "    \n",
    "    # Get the corresponding word from the vocabulary\n",
    "    predicted_word = vocab_array[top_index - 1]\n",
    "    \n",
    "    # Generate a suggestion\n",
    "    suggestion = f\"Did you mean '{input_text} {predicted_word}'?\"\n",
    "    \n",
    "    return suggestion\n",
    "\n",
    "# Example usage\n",
    "suggestion = suggest_next_word('sp1der man')\n",
    "print(suggestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Generated Movies: ['spider', 'man', 'Spider-Man 3', 'Spider-Man 2', 'Spider-Man 3', 'Spider-Man 3', 'Spider-Man 2']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process\n",
    "#Imperfect make_pred function which takes in a list of words and returns a list of predictions.\n",
    "def make_pred(text, n_words, beam_width=3, similarity_threshold=80):\n",
    "    predictions = []  # Initialize a list to store predictions\n",
    "    \n",
    "    # Split the input text into words\n",
    "    input_words = text.split()\n",
    "\n",
    "    for word in input_words:\n",
    "        if word in vocab_array:\n",
    "            # If a word in the input is a valid word, use it in predictions\n",
    "            predictions.append(word)\n",
    "        else:\n",
    "            # Find the nearest valid word using fuzzy string matching with lower threshold\n",
    "            best_match, similarity = process.extractOne(word, vocab_array, scorer=fuzz.ratio)\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                predictions.append(best_match)\n",
    "\n",
    "    # If there are valid words in the input, consider them for prediction\n",
    "    if predictions:\n",
    "        text = \" \".join(predictions)  # Use the words found in the input for prediction\n",
    "\n",
    "    # Initialize the beam with the best match\n",
    "    beam = [(text, 0)]\n",
    "    \n",
    "    for i in range(n_words):\n",
    "        new_beam = []\n",
    "        \n",
    "        for seq, score in beam:\n",
    "            text_tokenize = tokenizer.texts_to_sequences([seq])[-1]\n",
    "            text_padded = tf.keras.preprocessing.sequence.pad_sequences([text_tokenize], maxlen=14)\n",
    "            \n",
    "            # Predict the next word\n",
    "            prediction = model.predict(text_padded)[0]\n",
    "            \n",
    "            # Get the top predicted word indices\n",
    "            top_indices = np.argsort(prediction)[-beam_width:][::-1]\n",
    "            \n",
    "            # Get the corresponding words and probabilities from the vocabulary\n",
    "            top_words = [vocab_array[index - 1] for index in top_indices]\n",
    "            top_probs = [prediction[index] for index in top_indices]\n",
    "            \n",
    "            # Add the new sequences and their scores to the new beam\n",
    "            new_beam.extend([(seq + \" \" + word, score + np.log(prob)) for word, prob in zip(top_words, top_probs)])\n",
    "        \n",
    "        # Keep only the top sequences in the beam\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        # Check if any of the sequences in the beam are similar to an entry in the movie_name list\n",
    "        for seq, score in beam:\n",
    "            best_match, similarity = process.extractOne(seq, movie_name, scorer=fuzz.ratio)\n",
    "            if similarity >= similarity_threshold:\n",
    "                predictions.append(best_match)  # Add the movie name to the list\n",
    "    \n",
    "    if not predictions:  # If no similar sequences were found\n",
    "        predictions.append(movie_name[0])  # Add a default movie name\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "generated_texts = make_pred(\"Spider man\", 2)\n",
    "print(\"Generated Movies:\", generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m         predictions\u001b[39m.\u001b[39madd(movie_name[\u001b[39m0\u001b[39m])  \u001b[39m# Add a default movie name\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(predictions)) \n\u001b[1;32m---> 60\u001b[0m generated_texts \u001b[39m=\u001b[39m make_pred(\u001b[39m\"\u001b[39;49m\u001b[39mSpider man\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated Movies:\u001b[39m\u001b[39m\"\u001b[39m, generated_texts)\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mmake_pred\u001b[1;34m(text, n_words, beam_width, similarity_threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m input_words \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit()\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m input_words:\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m vocab_array:\n\u001b[0;32m     10\u001b[0m         \u001b[39m# If a word in the input is a valid word, use it in predictions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         predictions\u001b[39m.\u001b[39madd(word)\n\u001b[0;32m     12\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m         \u001b[39m# Find the nearest valid word using fuzzy string matching with a lower threshold\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_array' is not defined"
     ]
    }
   ],
   "source": [
    "def make_pred(text, n_words, beam_width=3, similarity_threshold=80):\n",
    "    predictions = set()  # Initialize a set to store unique predictions\n",
    "    added_predictions = set()  # Keep track of added predictions to avoid duplicates\n",
    "\n",
    "    # Split the input text into words\n",
    "    input_words = text.split()\n",
    "\n",
    "    for word in input_words:\n",
    "        if word in vocab_array:\n",
    "            # If a word in the input is a valid word, use it in predictions\n",
    "            predictions.add(word)\n",
    "        else:\n",
    "            # Find the nearest valid word using fuzzy string matching with a lower threshold\n",
    "            best_match, similarity = process.extractOne(word, vocab_array, scorer=fuzz.ratio)\n",
    "\n",
    "            if similarity >= similarity_threshold:\n",
    "                predictions.add(best_match)\n",
    "\n",
    "    # If there are valid words in the input, consider them for prediction\n",
    "    if predictions:\n",
    "        text = \" \".join(predictions)  # Use the words found in the input for prediction\n",
    "\n",
    "    # Initialize the beam with the best match\n",
    "    beam = [(text, 0)]\n",
    "\n",
    "    for i in range(n_words):\n",
    "        new_beam = []\n",
    "\n",
    "        for seq, score in beam:\n",
    "            text_tokenize = tokenizer.texts_to_sequences([seq])[-1]\n",
    "            text_padded = tf.keras.preprocessing.sequence.pad_sequences([text_tokenize], maxlen=14)\n",
    "\n",
    "            # Predict the next word\n",
    "            prediction = model.predict(text_padded)[0]\n",
    "\n",
    "            # Get the top predicted word indices\n",
    "            top_indices = np.argsort(prediction)[-beam_width:][::-1]\n",
    "\n",
    "            # Get the corresponding words and probabilities from the vocabulary\n",
    "            top_words = [vocab_array[index - 1] for index in top_indices]\n",
    "            top_probs = [prediction[index] for index in top_indices]\n",
    "\n",
    "            # Add the new sequences and their scores to the new beam\n",
    "            new_beam.extend([(seq + \" \" + word, score + np.log(prob)) for word, prob in zip(top_words, top_probs)])\n",
    "\n",
    "        # Keep only the top sequences in the beam\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # Check if any of the sequences in the beam are similar to an entry in the movie_name list\n",
    "        for seq, score in beam:\n",
    "            best_match, similarity = process.extractOne(seq, movie_name, scorer=fuzz.ratio)\n",
    "            if similarity >= similarity_threshold and best_match not in added_predictions:\n",
    "                predictions.add(best_match)\n",
    "                added_predictions.add(best_match)\n",
    "\n",
    "    if not predictions:  # If no similar sequences were found\n",
    "        predictions.add(movie_name[0])  # Add a default movie name\n",
    "\n",
    "    return sorted(list(predictions)) \n",
    "generated_texts = make_pred(\"Spider man\", 2)\n",
    "print(\"Generated Movies:\", generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import heapq\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_sim(str1, str2):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vec1 = vectorizer.fit_transform([str1]).toarray()\n",
    "    vec2 = vectorizer.transform([str2]).toarray()\n",
    "    sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "    return sim\n",
    "\n",
    "def make_pred(text, n_words, beam_width=3, similarity_threshold=0.5):\n",
    "    if text in vocab_array:\n",
    "        suggestion = f\"Did you mean '{text}'?\"\n",
    "    else:\n",
    "        best_match, similarity = process.extractOne(text, vocab_array, scorer=cosine_sim)\n",
    "        beam = [(0, best_match)]\n",
    "        \n",
    "        for i in range(n_words):\n",
    "            new_beam = []\n",
    "            \n",
    "            for score, seq in beam:\n",
    "                text_tokenize = tokenizer.texts_to_sequences([seq])[-1]\n",
    "                text_padded = tf.keras.preprocessing.sequence.pad_sequences([text_tokenize], maxlen=14)\n",
    "                \n",
    "                prediction = model.predict(text_padded)[0]\n",
    "                top_indices = np.argsort(prediction)[-beam_width:][::-1]\n",
    "                top_words = [vocab_array[index - 1] for index in top_indices]\n",
    "                top_probs = [prediction[index] for index in top_indices]\n",
    "                \n",
    "                new_beam.extend([(score + np.log(prob), seq + \" \" + word) for word, prob in zip(top_words, top_probs)])\n",
    "            \n",
    "            beam = heapq.nlargest(beam_width, new_beam)\n",
    "            \n",
    "            for score, seq in beam:\n",
    "                best_match, similarity = process.extractOne(seq, movie_name, scorer=cosine_sim)\n",
    "                if similarity >= similarity_threshold:\n",
    "                    return best_match\n",
    "        \n",
    "        generated_text = beam[0][1]\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "# Example usage\n",
    "generated_text = make_pred(\"Spid\", 7)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'of', 'a', ..., 'signed', 'sealed', 'delivered'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8599, 4797)\n",
      "4797\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23268"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# Example usage (replace this with your actual model and tokenizer)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 58\u001b[0m predicted_word \u001b[39m=\u001b[39m predict_next_word(user_input)\n\u001b[0;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicted Next Word: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_word\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mpredict_next_word\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_next_word\u001b[39m(input_text):\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Tokenize and pad the input_text (replace this with your actual tokenization and padding code)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     input_sequence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences([input_text])[\u001b[39m0\u001b[39m]\n\u001b[0;32m     43\u001b[0m     input_sequence \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39msequence\u001b[39m.\u001b[39mpad_sequences([input_sequence], maxlen\u001b[39m=\u001b[39m\u001b[39m14\u001b[39m)\n\u001b[0;32m     45\u001b[0m     \u001b[39m# Use the trained model to predict the next word\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Define a function to suggest valid movie titles\n",
    "def suggest_valid_movie_titles(input_text):\n",
    "    # Check if the input text is already a valid movie title\n",
    "    if input_text in movie_name:\n",
    "        return input_text\n",
    "    \n",
    "    # Use fuzzy string matching to find the nearest valid title\n",
    "    best_match, _ = process.extractOne(input_text, movie_name)\n",
    "    \n",
    "    # Check if the best match is similar enough\n",
    "    if fuzz.ratio(input_text, best_match) >= 80:\n",
    "        return best_match\n",
    "    \n",
    "    # Use text prediction to generate valid movie titles\n",
    "    generated_title = generate_movie_title(input_text)\n",
    "    \n",
    "    return generated_title\n",
    "\n",
    "# Define a function to generate valid movie titles using text prediction\n",
    "def generate_movie_title(input_text):\n",
    "    generated_title = input_text\n",
    "    \n",
    "    while True:\n",
    "        # Use your text prediction model to predict the next word\n",
    "        next_word = predict_next_word(generated_title)\n",
    "        \n",
    "        # Check if the generated text is a valid movie title\n",
    "        if next_word in movie_name:\n",
    "            generated_title += \" \" + next_word\n",
    "            break  # Stop generating words once a valid title is found\n",
    "        else:\n",
    "            generated_title += \" \" + next_word\n",
    "    \n",
    "    return generated_title\n",
    "\n",
    "# Define the predict_next_word function\n",
    "def predict_next_word(input_text):\n",
    "    # Tokenize and pad the input_text \n",
    "    input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    input_sequence = tf.keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=14)\n",
    "    \n",
    "    # Use the trained model to predict the next word\n",
    "    prediction = model.predict(input_sequence)\n",
    "    \n",
    "    # Find the index of the most likely word\n",
    "    predicted_word_index = np.argmax(prediction)\n",
    "    \n",
    "    # Map the index back to the actual word using your vocabulary\n",
    "    predicted_word = reverse_word_index[predicted_word_index]\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Example usage \n",
    "user_input = \"batm\"\n",
    "predicted_word = predict_next_word(user_input)\n",
    "\n",
    "print(f\"Predicted Next Word: {predicted_word}\")\n",
    "\n",
    "# Example usage\n",
    "user_input = \"batm\"\n",
    "suggested_movie_title = suggest_valid_movie_titles(user_input)\n",
    "\n",
    "if user_input == suggested_movie_title:\n",
    "    suggestion = f\"Did you mean '{user_input}'?\"\n",
    "else:\n",
    "    suggestion = f\"Did you mean '{user_input}'? Suggested Title: '{suggested_movie_title}'\"\n",
    "\n",
    "print(suggestion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
